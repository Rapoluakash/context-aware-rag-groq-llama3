
#ğŸ“š Context-Aware RAG using LLaMA-3 and Groq API

This project demonstrates a **context-aware Retrieval-Augmented Generation (RAG)** system using:

- **LLaMA-3 (70B)** via **Groq API**
- **FAISS** vector store
- **LangChain**
- **HuggingFace Embeddings**
- **PDF Loader** for document ingestion

---

#ğŸš€ Features

âœ… Loads and processes PDF files  
âœ… Splits documents into chunks  
âœ… Creates embeddings using HuggingFace's `all-MiniLM-L6-v2`  
âœ… Stores vector embeddings in FAISS  
âœ… Uses LLaMA-3 via Groq to answer questions  
âœ… Displays both answers and source references

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

bash
git clone https://github.com/your-username/context-aware-rag-groq-llama3.git
cd context-aware-rag-groq-llama3


### 2. Install Dependencies

Make sure youâ€™re using **Python 3.10+** and a virtual environment.

bash
pip install -r requirements.txt


#3. Place Your PDF

Place your `.pdf` file in the project directory and edit the script to point to it:

```python
pdf_path = "your_pdf_file.pdf"
```

#4. Add Your Groq API Key

In `rag_llama3_pdf_qa.py`, replace:

```python
os.environ["GROQ_API_KEY"] = "your_groq_api_key_here"
```

with your actual [Groq API Key](https://console.groq.com/keys).

---

#ğŸ§  Run the RAG Pipeline

```bash
python rag_llama3_pdf_qa.py
```

It will:

* Load the PDF
* Chunk and embed it
* Store embeddings
* Use LLaMA-3 to answer a query
* Print the answer and source document chunks

---

#ğŸ§ª Example Query

```python
query = "What are the main functions discussed in this document?"
```

**Output:**

```
Answer:
The document explains X, Y, and Z functions...

Sources:
â€¢ Page 3, your_pdf_file.pdf
â€¢ Page 7, your_pdf_file.pdf
```

---

#ğŸ“¦ File Structure

```
context-aware-rag-groq-llama3/
â”œâ”€â”€ rag_llama3_pdf_qa.py           # Main RAG pipeline code
â”œâ”€â”€ requirements.txt               # Required Python packages
â””â”€â”€ README.md                      # Project documentation
```

---

#ğŸ¤– LLM Details

* **Model:** `llama3-70b-8192`
* **Provider:** [Groq](https://console.groq.com/)
* **Speed:** Ultra-fast inference (<1ms/token with Groq)

---

#ğŸ” Credits

* [LangChain](https://www.langchain.com/)
* [Groq](https://groq.com/)
* [Meta LLaMA-3](https://ai.meta.com/llama/)
* [HuggingFace Embeddings](https://huggingface.co/sentence-transformers)

---

## ğŸ“¬ License

MIT License - Free to use and modify.

